{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3861498,"sourceType":"datasetVersion","datasetId":2296266}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/huynguyentran/SupContrast.git","metadata":{"id":"S0XrrZyC6Is0","outputId":"fecc3e84-273f-45e6-974b-2dc2c8ba3ab3","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T01:10:49.801291Z","iopub.execute_input":"2025-03-07T01:10:49.801524Z","iopub.status.idle":"2025-03-07T01:10:50.928730Z","shell.execute_reply.started":"2025-03-07T01:10:49.801502Z","shell.execute_reply":"2025-03-07T01:10:50.927614Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'SupContrast'...\nremote: Enumerating objects: 340, done.\u001b[K\nremote: Counting objects: 100% (106/106), done.\u001b[K\nremote: Compressing objects: 100% (54/54), done.\u001b[K\nremote: Total 340 (delta 81), reused 45 (delta 45), pack-reused 234 (from 4)\u001b[K\nReceiving objects: 100% (340/340), 1.52 MiB | 11.40 MiB/s, done.\nResolving deltas: 100% (177/177), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install tensorboard-logger","metadata":{"id":"YI6qNt4M6cyD","outputId":"dd524ddc-05d5-44ae-ff34-12d13de5ea32","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T01:10:50.929775Z","iopub.execute_input":"2025-03-07T01:10:50.930000Z","iopub.status.idle":"2025-03-07T01:10:56.884061Z","shell.execute_reply.started":"2025-03-07T01:10:50.929978Z","shell.execute_reply":"2025-03-07T01:10:56.882577Z"}},"outputs":[{"name":"stdout","text":"Collecting tensorboard-logger\n  Downloading tensorboard_logger-0.1.0-py2.py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from tensorboard-logger) (3.20.3)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorboard-logger) (1.17.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboard-logger) (1.26.4)\nRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard-logger) (1.13.1)\nRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard-logger) (11.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboard-logger) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboard-logger) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboard-logger) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboard-logger) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboard-logger) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboard-logger) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->tensorboard-logger) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->tensorboard-logger) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->tensorboard-logger) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->tensorboard-logger) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->tensorboard-logger) (2024.2.0)\nDownloading tensorboard_logger-0.1.0-py2.py3-none-any.whl (17 kB)\nInstalling collected packages: tensorboard-logger\nSuccessfully installed tensorboard-logger-0.1.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sys\n# sys.path.append(\"/kaggle/working\")\nsys.path.append(\"SupContrast\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T01:10:56.885962Z","iopub.execute_input":"2025-03-07T01:10:56.886368Z","iopub.status.idle":"2025-03-07T01:10:56.891343Z","shell.execute_reply.started":"2025-03-07T01:10:56.886333Z","shell.execute_reply":"2025-03-07T01:10:56.890498Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from __future__ import print_function\nimport os\nimport argparse\nimport time\nimport math\nimport importlib\n\nimport tensorboard_logger as tb_logger\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torchvision import transforms, datasets\n# import SupContrast.networks.resnet_big\n# importlib.reload(SupContrast.networks.resnet_big)\n\nimport networks.resnet_big\nimportlib.reload(networks.resnet_big)\n\nfrom SupContrast.util import TwoCropTransform, AverageMeter\nfrom SupContrast.util import adjust_learning_rate, warmup_learning_rate\nfrom SupContrast.util import set_optimizer, save_model\nfrom SupContrast.networks.resnet_big import SupConViT, SupConResNet, CrossAttViT\nfrom SupContrast.losses import SupConLoss\n\n\n\ntry:\n    import apex\n    from apex import amp, optimizers\nexcept ImportError:\n    pass\n\n\ndef parse_option():\n    parser = argparse.ArgumentParser('argument for training')\n\n    parser.add_argument('--print_freq', type=int, default=10,\n                        help='print frequency')\n    parser.add_argument('--save_freq', type=int, default=50,\n                        help='save frequency')\n    parser.add_argument('--batch_size', type=int, default=256,\n                        help='batch_size')\n    parser.add_argument('--num_workers', type=int, default=16,\n                        help='num of workers to use')\n    parser.add_argument('--epochs', type=int, default=1000,\n                        help='number of training epochs')\n\n    # optimization\n    parser.add_argument('--learning_rate', type=float, default=0.05,\n                        help='learning rate')\n    parser.add_argument('--lr_decay_epochs', type=str, default='700,800,900',\n                        help='where to decay lr, can be a list')\n    parser.add_argument('--lr_decay_rate', type=float, default=0.1,\n                        help='decay rate for learning rate')\n    parser.add_argument('--weight_decay', type=float, default=1e-4,\n                        help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        help='momentum')\n\n    # model dataset\n    parser.add_argument('--model', type=str, default='resnet50')\n    parser.add_argument('--dataset', type=str, default='cifar10',\n                        choices=['cifar10', 'cifar100', 'path'], help='dataset')\n    parser.add_argument('--mean', type=str, help='mean of dataset in path in form of str tuple')\n    parser.add_argument('--std', type=str, help='std of dataset in path in form of str tuple')\n    parser.add_argument('--data_folder', type=str, default=None, help='path to custom dataset')\n    parser.add_argument('--size', type=int, default=32, help='parameter for RandomResizedCrop')\n\n    # method\n    parser.add_argument('--method', type=str, default='SupCon',\n                        choices=['SupCon', 'SimCLR'], help='choose method')\n\n    # temperature\n    parser.add_argument('--temp', type=float, default=0.07,\n                        help='temperature for loss function')\n    \n    parser.add_argument('--optimizer_name', type=str, default=\"adamw\",\n                        help='optimizer of the funciton')\n\n    # other setting\n    parser.add_argument('--cosine', action='store_true',\n                        help='using cosine annealing')\n    parser.add_argument('--syncBN', action='store_true',\n                        help='using synchronized batch normalization')\n    parser.add_argument('--warm', action='store_true',\n                        help='warm-up for large batch training')\n    parser.add_argument('--trial', type=str, default='0',\n                        help='id for recording multiple runs')\n\n    opt = parser.parse_args()\n\n    # check if dataset is path that passed required arguments\n    if opt.dataset == 'path':\n        assert opt.data_folder is not None \\\n            and opt.mean is not None \\\n            and opt.std is not None\n\n    # set the path according to the environment\n    if opt.data_folder is None:\n        opt.data_folder = './datasets/'\n    opt.model_path = './save/SupCon/{}_models'.format(opt.dataset)\n    opt.tb_path = './save/SupCon/{}_tensorboard'.format(opt.dataset)\n\n    iterations = opt.lr_decay_epochs.split(',')\n    opt.lr_decay_epochs = list([])\n    for it in iterations:\n        opt.lr_decay_epochs.append(int(it))\n\n    opt.model_name = '{}_{}_{}_lr_{}_decay_{}_bsz_{}_temp_{}_trial_{}'.\\\n        format(opt.method, opt.dataset, opt.model, opt.learning_rate,\n               opt.weight_decay, opt.batch_size, opt.temp, opt.trial)\n\n    if opt.cosine:\n        opt.model_name = '{}_cosine'.format(opt.model_name)\n\n    # warm-up for large-batch training,\n    if opt.batch_size > 256:\n        opt.warm = True\n    if opt.warm:\n        opt.model_name = '{}_warm'.format(opt.model_name)\n        opt.warmup_from = 0.01\n        opt.warm_epochs = 10\n        if opt.cosine:\n            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n        else:\n            opt.warmup_to = opt.learning_rate\n\n    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n    if not os.path.isdir(opt.tb_folder):\n        os.makedirs(opt.tb_folder)\n\n    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n    if not os.path.isdir(opt.save_folder):\n        os.makedirs(opt.save_folder)\n\n    return opt\n\n\ndef set_loader(opt):\n    # construct data loader\n    if opt.dataset == 'cifar10':\n        mean = (0.4914, 0.4822, 0.4465)\n        std = (0.2023, 0.1994, 0.2010)\n    elif opt.dataset == 'cifar100':\n        mean = (0.5071, 0.4867, 0.4408)\n        std = (0.2675, 0.2565, 0.2761)\n    elif opt.dataset == 'path':\n        mean = eval(opt.mean)\n        std = eval(opt.std)\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n    normalize = transforms.Normalize(mean=mean, std=std)\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=opt.size, scale=(0.2, 1.)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomApply([\n            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n        ], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    if opt.dataset == 'cifar10':\n        train_dataset = datasets.CIFAR10(root=opt.data_folder,\n                                         transform=TwoCropTransform(train_transform),\n                                         download=True)\n    elif opt.dataset == 'cifar100':\n        train_dataset = datasets.CIFAR100(root=opt.data_folder,\n                                          transform=TwoCropTransform(train_transform),\n                                          download=True)\n    elif opt.dataset == 'path':\n        train_dataset = datasets.ImageFolder(root=opt.data_folder,\n                                            transform=TwoCropTransform(train_transform))\n    else:\n        raise ValueError(opt.dataset)\n\n    train_sampler = None\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=opt.batch_size, shuffle=(train_sampler is None),\n        num_workers=opt.num_workers, pin_memory=True, sampler=train_sampler)\n\n    return train_loader\n\n\ndef set_model(opt):\n    model = CrossAttViT(name=opt.model)\n    # if 'vit' in opt.model:\n    #     model = SupConViT(name=opt.model)\n    # else:\n    #     model = SupConResNet(name=opt.model)\n    criterion = SupConLoss(temperature=opt.temp)\n\n    # enable synchronized Batch Normalization\n    if opt.syncBN:\n        model = apex.parallel.convert_syncbn_model(model)\n\n    if torch.cuda.is_available():\n        if torch.cuda.device_count() > 1:\n            model.encoder = torch.nn.DataParallel(model.encoder)\n        device = \"cuda\"\n        model = model.to(device)\n        criterion = criterion.to(device)\n        cudnn.benchmark = True\n\n    return model, criterion","metadata":{"id":"13f-ubWjUhSg","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T01:10:56.892944Z","iopub.execute_input":"2025-03-07T01:10:56.893266Z","iopub.status.idle":"2025-03-07T01:14:14.155164Z","shell.execute_reply.started":"2025-03-07T01:10:56.893232Z","shell.execute_reply":"2025-03-07T01:14:14.154182Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n  warnings.warn(\"xFormers is not available (SwiGLU)\")\n/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n  warnings.warn(\"xFormers is not available (Attention)\")\n/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n  warnings.warn(\"xFormers is not available (Block)\")\nDownloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_reg4_pretrain.pth\n100%|██████████| 84.2M/84.2M [00:00<00:00, 109MB/s] \nUsing cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\nDownloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitb14_reg4_pretrain.pth\n100%|██████████| 330M/330M [00:02<00:00, 147MB/s]  \nUsing cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\nDownloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitl14_reg4_pretrain.pth\n100%|██████████| 1.13G/1.13G [00:07<00:00, 155MB/s] \nUsing cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\nDownloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitg14_reg4_pretrain.pth\n100%|██████████| 4.23G/4.23G [01:25<00:00, 53.0MB/s]\nUsing cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\nUsing cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\nUsing cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\nUsing cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\ndef train(train_loader, model, criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    end = time.time()\n    for idx, (images, labels) in enumerate(train_loader):\n        if idx ==3:\n            break\n        data_time.update(time.time() - end)\n\n        # images = torch.cat([images[0], images[1]], dim=0)\n        img1, img2 = images[0], images[1]\n        if torch.cuda.is_available():\n            # images = images.cuda(non_blocking=True)\n            img1 = img1.cuda(non_blocking=True)\n            img2 = img2.cuda(non_blocking= True)\n            labels = labels.cuda(non_blocking=True)\n        bsz = labels.shape[0]\n\n        # warm-up learning rate\n        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n\n        # compute loss\n        # features = model(images)\n  \n        # f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n        f1,f2 = model(img1, img2)\n        features = torch.stack([f1, f2], dim=1)  \n\n\n   \n        \n        if opt.method == 'SupCon':\n            loss = criterion(features, labels)\n        elif opt.method == 'SimCLR':\n            loss = criterion(features)\n        else:\n            raise ValueError('contrastive method not supported: {}'.\n                             format(opt.method))\n       \n        # print(f\"Loss: {loss.item()}\")\n\n        \n        # update metric\n        losses.update(loss.item(), bsz)\n\n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % opt.print_freq == 0:\n            print('Train: [{0}][{1}/{2}]\\t'\n                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'loss {loss.val:.3f} ({loss.avg:.3f})'.format(\n                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses))\n            sys.stdout.flush()\n\n    return losses.avg\n\n\ndef main(opt):\n    # opt = parse_option()\n\n    # build data loader\n    train_loader = set_loader(opt)\n\n    # build model and criterion\n    model, criterion = set_model(opt)\n\n    # build optimizer\n    optimizer = set_optimizer(opt, model)\n\n    # tensorboard\n    # logger = tb_logger.Logger(logdir=opt.tb_folder, flush_secs=2)\n\n    # training routine\n    for epoch in range(1, opt.epochs + 1):\n        adjust_learning_rate(opt, optimizer, epoch)\n\n        # train for one epoch\n        time1 = time.time()\n        loss = train(train_loader, model, criterion, optimizer, epoch, opt)\n        time2 = time.time()\n        print('epoch {}, loss {:.4f}, total time {:.2f}s'.format(epoch, loss, time2 - time1))\n\n        # # tensorboard logger\n        # logger.log_value('loss', loss, epoch)\n        # logger.log_value('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n\n        if epoch % opt.save_freq == 0:\n            save_file = os.path.join(\n                opt.save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n            save_model(model, optimizer, opt, epoch, save_file)\n\n    # save the last model\n    save_file = os.path.join(\n        opt.save_folder, 'last.pth')\n    save_model(model, optimizer, opt, opt.epochs, save_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T01:14:14.156733Z","iopub.execute_input":"2025-03-07T01:14:14.157398Z","iopub.status.idle":"2025-03-07T01:14:14.171222Z","shell.execute_reply.started":"2025-03-07T01:14:14.157365Z","shell.execute_reply":"2025-03-07T01:14:14.170004Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport math\nimport argparse\nimport time\nimport torch\n\n# Define an Opt class to hold the configuration\nclass Opt:\n    def __init__(self, **kwargs):\n        # Default values as per the parse_option function\n        self.print_freq = kwargs.get('print_freq', 10)\n        self.save_freq = kwargs.get('save_freq', 50)\n        self.batch_size = kwargs.get('batch_size', 256)\n        self.num_workers = kwargs.get('num_workers', 16)\n        self.epochs = kwargs.get('epochs', 1000)\n\n        # optimization\n        self.learning_rate = kwargs.get('learning_rate', 0.05)\n        self.lr_decay_epochs = kwargs.get('lr_decay_epochs', '700,800,900')\n        self.lr_decay_rate = kwargs.get('lr_decay_rate', 0.1)\n        self.weight_decay = kwargs.get('weight_decay', 1e-4)\n        self.momentum = kwargs.get('momentum', 0.9)\n\n        # model dataset\n        self.model = kwargs.get('model', 'resnet50')\n        self.dataset = kwargs.get('dataset', 'cifar10')\n        self.mean = kwargs.get('mean', None)\n        self.std = kwargs.get('std', None)\n        self.data_folder = kwargs.get('data_folder', None)\n        self.size = kwargs.get('size', 32)\n\n        # method\n        self.method = kwargs.get('method', 'SupCon')\n\n        # temperature\n        self.temp = kwargs.get('temp', 0.07)\n\n        # other settings\n        self.cosine = kwargs.get('cosine', False)\n        self.syncBN = kwargs.get('syncBN', False)\n        self.warm = kwargs.get('warm', False)\n        self.trial = kwargs.get('trial', '0')\n        self.optimizer_name = kwargs.get('optimizer_name', 'adamw')\n\n        # Check if dataset is 'path' and passed required arguments\n        if self.dataset == 'path':\n            assert self.data_folder is not None and self.mean is not None and self.std is not None\n\n        # Set the path according to the environment\n        if self.data_folder is None:\n            self.data_folder = './datasets/'\n\n        self.model_path = './save/SupCon/{}_models'.format(self.dataset)\n        self.tb_path = './save/SupCon/{}_tensorboard'.format(self.dataset)\n\n        # Handle lr_decay_epochs\n        iterations = self.lr_decay_epochs.split(',')\n        self.lr_decay_epochs = [int(it) for it in iterations]\n\n        # Prepare model name\n        self.model_name = '{}_{}_{}_lr_{}_decay_{}_bsz_{}_temp_{}_trial_{}'.format(\n            self.method, self.dataset, self.model, self.learning_rate,\n            self.weight_decay, self.batch_size, self.temp, self.trial)\n\n        if self.cosine:\n            self.model_name = '{}_cosine'.format(self.model_name)\n\n        # Warm-up for large-batch training\n        if self.batch_size > 256:\n            self.warm = True\n        if self.warm:\n            self.model_name = '{}_warm'.format(self.model_name)\n            self.warmup_from = 0.01\n            self.warm_epochs = 10\n            if self.cosine:\n                eta_min = self.learning_rate * (self.lr_decay_rate ** 3)\n                self.warmup_to = eta_min + (self.learning_rate - eta_min) * (\n                        1 + math.cos(math.pi * self.warm_epochs / self.epochs)) / 2\n            else:\n                self.warmup_to = self.learning_rate\n\n        self.tb_folder = os.path.join(self.tb_path, self.model_name)\n        if not os.path.isdir(self.tb_folder):\n            os.makedirs(self.tb_folder)\n\n        self.save_folder = os.path.join(self.model_path, self.model_name)\n        if not os.path.isdir(self.save_folder):\n            os.makedirs(self.save_folder)\n\n# torch.cuda.empty_cache()\n# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\nopt = Opt(\n    batch_size=8,\n    learning_rate=1e-5, \n    print_freq=100,\n    save_freq=100,\n    temp=0.1,\n    cosine=True,\n    num_workers=4,\n    dataset=\"path\",\n    data_folder=\"/kaggle/input/identification-of-pseudopapilledema\",\n    mean= \"(0.4914, 0.4822, 0.4465)\",  \n    std=\"(0.2675, 0.2565, 0.2761)\",   \n    epochs=100,\n    # lr_decay_epochs=\"5,7,9\",  \n    # lr_decay_rate=0.1,\n    # weight_decay=5e-2, \n    size=224,\n    optimizer_name='adamw',\n    model='vit_l_dino',\n    warm=True,  \n)\n\nmain(opt)","metadata":{"id":"yu5OVrd4Vv6m","outputId":"9543fa74-e18f-415e-ec5e-da704eccbfdd","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T01:31:50.864104Z","iopub.execute_input":"2025-03-07T01:31:50.864488Z"}},"outputs":[{"name":"stdout","text":"epoch 1, loss 291.3969, total time 11.53s\nepoch 2, loss 33.8866, total time 7.63s\nepoch 3, loss 50.7222, total time 7.51s\nepoch 4, loss 74.9190, total time 7.68s\nepoch 5, loss 74.6335, total time 7.89s\nepoch 6, loss 84.1577, total time 7.99s\nepoch 7, loss 56.1812, total time 7.83s\nepoch 8, loss 55.4556, total time 7.81s\nepoch 9, loss 49.7128, total time 8.31s\nepoch 10, loss 38.3422, total time 8.09s\nepoch 11, loss 20.0096, total time 8.12s\nepoch 12, loss 20.8223, total time 8.17s\nepoch 13, loss 20.4152, total time 8.57s\nepoch 14, loss 20.3850, total time 8.31s\nepoch 15, loss 21.0373, total time 8.41s\nepoch 16, loss 21.0373, total time 8.57s\nepoch 17, loss 20.7920, total time 8.78s\nepoch 18, loss 20.9734, total time 8.76s\nepoch 19, loss 20.2437, total time 8.85s\nepoch 20, loss 20.7920, total time 8.82s\nepoch 21, loss 21.0373, total time 8.71s\nepoch 22, loss 20.6333, total time 8.46s\nepoch 23, loss 20.5947, total time 8.40s\nepoch 24, loss 21.0146, total time 8.41s\nepoch 25, loss 19.9617, total time 8.63s\nepoch 26, loss 19.9825, total time 8.62s\nepoch 27, loss 20.6269, total time 8.92s\nepoch 28, loss 20.4379, total time 8.76s\nepoch 29, loss 20.7838, total time 8.53s\nepoch 30, loss 21.0037, total time 8.53s\nepoch 31, loss 20.1907, total time 8.60s\nepoch 32, loss 19.2157, total time 8.66s\nepoch 33, loss 20.7838, total time 8.52s\nepoch 34, loss 20.3989, total time 8.53s\nepoch 35, loss 20.3989, total time 8.64s\nepoch 36, loss 20.0135, total time 8.69s\nepoch 37, loss 20.6250, total time 8.60s\nepoch 38, loss 21.2262, total time 8.88s\nepoch 39, loss 20.3611, total time 8.90s\nepoch 40, loss 21.3885, total time 8.63s\nepoch 41, loss 19.1628, total time 8.64s\nepoch 42, loss 20.1722, total time 8.62s\nepoch 43, loss 20.6559, total time 8.75s\nepoch 44, loss 20.4360, total time 8.54s\nepoch 45, loss 20.7920, total time 8.59s\nepoch 46, loss 20.0257, total time 8.76s\nepoch 47, loss 20.6560, total time 8.70s\nepoch 48, loss 20.7920, total time 8.56s\nepoch 49, loss 19.7901, total time 8.74s\nepoch 50, loss 20.5913, total time 8.92s\nepoch 51, loss 19.8141, total time 8.60s\nepoch 52, loss 19.3634, total time 8.61s\nepoch 53, loss 21.0373, total time 8.64s\nepoch 54, loss 20.5967, total time 8.72s\nepoch 55, loss 20.6269, total time 8.51s\nepoch 56, loss 20.7837, total time 8.60s\nepoch 57, loss 19.9954, total time 8.70s\nepoch 58, loss 20.5966, total time 8.51s\nepoch 59, loss 20.0547, total time 8.56s\nepoch 60, loss 20.1873, total time 8.59s\nepoch 61, loss 20.3687, total time 8.87s\nepoch 62, loss 21.0071, total time 8.65s\nepoch 63, loss 19.1965, total time 8.58s\nepoch 64, loss 20.8193, total time 8.56s\nepoch 65, loss 20.8146, total time 8.80s\nepoch 66, loss 19.3495, total time 8.58s\nepoch 67, loss 20.3687, total time 8.68s\nepoch 68, loss 20.8146, total time 8.90s\nepoch 69, loss 21.0146, total time 8.59s\nepoch 70, loss 19.8396, total time 8.56s\nepoch 71, loss 20.4216, total time 8.61s\nepoch 72, loss 20.1873, total time 8.79s\nepoch 73, loss 20.2024, total time 8.54s\nepoch 74, loss 20.4216, total time 8.53s\nepoch 75, loss 19.9919, total time 8.83s\nepoch 76, loss 20.9431, total time 8.59s\nepoch 77, loss 20.2134, total time 8.59s\nepoch 78, loss 20.7920, total time 8.65s\nepoch 79, loss 20.9844, total time 8.77s\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# opt = Opt(\n#     batch_size=32,\n#     learning_rate=0.5, \n#     save_freq =10,\n#     temp=0.1,\n#     cosine=True,\n#     num_workers=4,\n#     dataset=\"path\",\n#     data_folder=\"/kaggle/input/identification-of-pseudopapilledema\",\n#     mean=\"(0.4914, 0.4822, 0.4465)\",\n#     std=\"(0.2675, 0.2565, 0.2761)\",\n#     method=\"SupCon\",\n#     size = 224,\n#     model='vit_s_dino',\n# )\n\n\n\n# opt = Opt(\n#     batch_size=32,\n#     learning_rate=0.5, \n#     save_freq =10,\n#     temp=0.1,\n#     cosine=True,\n#     num_workers=4,\n#     dataset=\"path\",\n#     data_folder=\"/kaggle/input/identification-of-pseudopapilledema\",\n#     mean=\"(0.4914, 0.4822, 0.4465)\",\n#     std=\"(0.2675, 0.2565, 0.2761)\",\n#     method=\"SupCon\",\n#     size = 32,\n#     model='resnet50',\n# )\n\n# main(opt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:49:34.283263Z","iopub.status.idle":"2025-03-05T07:49:34.283594Z","shell.execute_reply":"2025-03-05T07:49:34.283476Z"}},"outputs":[],"execution_count":null}]}