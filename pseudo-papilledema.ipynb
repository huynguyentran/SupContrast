{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3861498,"sourceType":"datasetVersion","datasetId":2296266}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/huynguyentran/SupContrast.git","metadata":{"id":"S0XrrZyC6Is0","outputId":"fecc3e84-273f-45e6-974b-2dc2c8ba3ab3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorboard-logger","metadata":{"id":"YI6qNt4M6cyD","outputId":"dd524ddc-05d5-44ae-ff34-12d13de5ea32","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\n# sys.path.append(\"/kaggle/working\")\nsys.path.append(\"SupContrast\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from __future__ import print_function\nimport os\nimport argparse\nimport time\nimport math\nimport importlib\n\nimport tensorboard_logger as tb_logger\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torchvision import transforms, datasets\n# import SupContrast.networks.resnet_big\n# importlib.reload(SupContrast.networks.resnet_big)\n\nimport networks.resnet_big\nimportlib.reload(networks.resnet_big)\n\nfrom SupContrast.util import TwoCropTransform, AverageMeter\nfrom SupContrast.util import adjust_learning_rate, warmup_learning_rate\nfrom SupContrast.util import set_optimizer, save_model\nfrom SupContrast.networks.resnet_big import SupConViT, SupConResNet, CrossAttViT\nfrom SupContrast.losses import SupConLoss\n\n\n\ntry:\n    import apex\n    from apex import amp, optimizers\nexcept ImportError:\n    pass\n\n\ndef parse_option():\n    parser = argparse.ArgumentParser('argument for training')\n\n    parser.add_argument('--print_freq', type=int, default=10,\n                        help='print frequency')\n    parser.add_argument('--save_freq', type=int, default=50,\n                        help='save frequency')\n    parser.add_argument('--batch_size', type=int, default=256,\n                        help='batch_size')\n    parser.add_argument('--num_workers', type=int, default=16,\n                        help='num of workers to use')\n    parser.add_argument('--epochs', type=int, default=1000,\n                        help='number of training epochs')\n\n    # optimization\n    parser.add_argument('--learning_rate', type=float, default=0.05,\n                        help='learning rate')\n    parser.add_argument('--lr_decay_epochs', type=str, default='700,800,900',\n                        help='where to decay lr, can be a list')\n    parser.add_argument('--lr_decay_rate', type=float, default=0.1,\n                        help='decay rate for learning rate')\n    parser.add_argument('--weight_decay', type=float, default=1e-4,\n                        help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        help='momentum')\n\n    # model dataset\n    parser.add_argument('--model', type=str, default='resnet50')\n    parser.add_argument('--dataset', type=str, default='cifar10',\n                        choices=['cifar10', 'cifar100', 'path'], help='dataset')\n    parser.add_argument('--mean', type=str, help='mean of dataset in path in form of str tuple')\n    parser.add_argument('--std', type=str, help='std of dataset in path in form of str tuple')\n    parser.add_argument('--data_folder', type=str, default=None, help='path to custom dataset')\n    parser.add_argument('--size', type=int, default=32, help='parameter for RandomResizedCrop')\n\n    # method\n    parser.add_argument('--method', type=str, default='SupCon',\n                        choices=['SupCon', 'SimCLR'], help='choose method')\n\n    # temperature\n    parser.add_argument('--temp', type=float, default=0.07,\n                        help='temperature for loss function')\n    \n    parser.add_argument('--optimizer_name', type=str, default=\"adamw\",\n                        help='optimizer of the funciton')\n\n    # other setting\n    parser.add_argument('--cosine', action='store_true',\n                        help='using cosine annealing')\n    parser.add_argument('--syncBN', action='store_true',\n                        help='using synchronized batch normalization')\n    parser.add_argument('--warm', action='store_true',\n                        help='warm-up for large batch training')\n    parser.add_argument('--trial', type=str, default='0',\n                        help='id for recording multiple runs')\n\n    opt = parser.parse_args()\n\n    # check if dataset is path that passed required arguments\n    if opt.dataset == 'path':\n        assert opt.data_folder is not None \\\n            and opt.mean is not None \\\n            and opt.std is not None\n\n    # set the path according to the environment\n    if opt.data_folder is None:\n        opt.data_folder = './datasets/'\n    opt.model_path = './save/SupCon/{}_models'.format(opt.dataset)\n    opt.tb_path = './save/SupCon/{}_tensorboard'.format(opt.dataset)\n\n    iterations = opt.lr_decay_epochs.split(',')\n    opt.lr_decay_epochs = list([])\n    for it in iterations:\n        opt.lr_decay_epochs.append(int(it))\n\n    opt.model_name = '{}_{}_{}_lr_{}_decay_{}_bsz_{}_temp_{}_trial_{}'.\\\n        format(opt.method, opt.dataset, opt.model, opt.learning_rate,\n               opt.weight_decay, opt.batch_size, opt.temp, opt.trial)\n\n    if opt.cosine:\n        opt.model_name = '{}_cosine'.format(opt.model_name)\n\n    # warm-up for large-batch training,\n    if opt.batch_size > 256:\n        opt.warm = True\n    if opt.warm:\n        opt.model_name = '{}_warm'.format(opt.model_name)\n        opt.warmup_from = 0.01\n        opt.warm_epochs = 10\n        if opt.cosine:\n            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n        else:\n            opt.warmup_to = opt.learning_rate\n\n    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n    if not os.path.isdir(opt.tb_folder):\n        os.makedirs(opt.tb_folder)\n\n    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n    if not os.path.isdir(opt.save_folder):\n        os.makedirs(opt.save_folder)\n\n    return opt\n\n\ndef set_loader(opt):\n    # construct data loader\n    if opt.dataset == 'cifar10':\n        mean = (0.4914, 0.4822, 0.4465)\n        std = (0.2023, 0.1994, 0.2010)\n    elif opt.dataset == 'cifar100':\n        mean = (0.5071, 0.4867, 0.4408)\n        std = (0.2675, 0.2565, 0.2761)\n    elif opt.dataset == 'path':\n        mean = eval(opt.mean)\n        std = eval(opt.std)\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n    normalize = transforms.Normalize(mean=mean, std=std)\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=opt.size, scale=(0.2, 1.)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomApply([\n            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n        ], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    if opt.dataset == 'cifar10':\n        train_dataset = datasets.CIFAR10(root=opt.data_folder,\n                                         transform=TwoCropTransform(train_transform),\n                                         download=True)\n    elif opt.dataset == 'cifar100':\n        train_dataset = datasets.CIFAR100(root=opt.data_folder,\n                                          transform=TwoCropTransform(train_transform),\n                                          download=True)\n    elif opt.dataset == 'path':\n        train_dataset = datasets.ImageFolder(root=opt.data_folder,\n                                            transform=TwoCropTransform(train_transform))\n    else:\n        raise ValueError(opt.dataset)\n    selected_classes = {\"Pseudopapilledema\", \"Papilledema\"}\n    print(selected_classes)\n    print(train_dataset.classes)\n    filtered_indices = [i for i, (_, label) in enumerate(train_dataset) if train_dataset.classes[label] in selected_classes]\n    filtered_dataset = torch.utils.data.Subset(train_dataset, filtered_indices)\n    train_sampler = None\n    train_loader = torch.utils.data.DataLoader(\n        filtered_dataset, batch_size=opt.batch_size, shuffle=(train_sampler is None),\n        num_workers=opt.num_workers, pin_memory=True, sampler=train_sampler)\n    seen_classes = set()\n    for images, labels in train_loader:\n        for label in labels:\n            seen_classes.add(train_dataset.classes[label.item()])  # Add the class name\n    print(\"Classes present in this train_loader:\", seen_classes)\n    return train_loader\n\n\ndef set_model(opt):\n    model = CrossAttViT(name=opt.model)\n    # if 'vit' in opt.model:\n    #     model = SupConViT(name=opt.model)\n    # else:\n    #     model = SupConResNet(name=opt.model)\n    criterion = SupConLoss(temperature=opt.temp)\n\n    # enable synchronized Batch Normalization\n    if opt.syncBN:\n        model = apex.parallel.convert_syncbn_model(model)\n\n    if torch.cuda.is_available():\n        if torch.cuda.device_count() > 1:\n            model.encoder = torch.nn.DataParallel(model.encoder)\n        device = \"cuda\"\n        model = model.to(device)\n        criterion = criterion.to(device)\n        cudnn.benchmark = True\n\n    return model, criterion","metadata":{"id":"13f-ubWjUhSg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef train(train_loader, model, criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    end = time.time()\n    for idx, (images, labels) in enumerate(train_loader):\n        if idx ==3:\n            break\n        data_time.update(time.time() - end)\n\n        images = torch.cat([images[0], images[1]], dim=0)\n        if torch.cuda.is_available():\n            images = images.cuda(non_blocking=True)\n            labels = labels.cuda(non_blocking=True)\n        bsz = labels.shape[0]\n\n        # warm-up learning rate\n        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n\n        # compute loss\n        # features = model(images)\n  \n        # f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n        f1,f2 = model(images, bsz)\n        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n\n\n   \n        \n        if opt.method == 'SupCon':\n            loss = criterion(features, labels)\n        elif opt.method == 'SimCLR':\n            loss = criterion(features)\n        else:\n            raise ValueError('contrastive method not supported: {}'.\n                             format(opt.method))\n       \n        # print(f\"Loss: {loss.item()}\")\n\n        \n        # update metric\n        losses.update(loss.item(), bsz)\n\n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % opt.print_freq == 0:\n            print('Train: [{0}][{1}/{2}]\\t'\n                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'loss {loss.val:.3f} ({loss.avg:.3f})'.format(\n                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses))\n            sys.stdout.flush()\n\n    return losses.avg\n\n\ndef main(opt):\n    # opt = parse_option()\n\n    # build data loader\n    train_loader = set_loader(opt)\n\n    # build model and criterion\n    model, criterion = set_model(opt)\n\n    # build optimizer\n    optimizer = set_optimizer(opt, model)\n\n    # tensorboard\n    # logger = tb_logger.Logger(logdir=opt.tb_folder, flush_secs=2)\n\n    # training routine\n    for epoch in range(1, opt.epochs + 1):\n        adjust_learning_rate(opt, optimizer, epoch)\n\n        # train for one epoch\n        time1 = time.time()\n        loss = train(train_loader, model, criterion, optimizer, epoch, opt)\n        time2 = time.time()\n        print('epoch {}, loss {:.4f}, total time {:.2f}s'.format(epoch, loss, time2 - time1))\n\n        # # tensorboard logger\n        # logger.log_value('loss', loss, epoch)\n        # logger.log_value('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n\n        if epoch % opt.save_freq == 0:\n            save_file = os.path.join(\n                opt.save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n            save_model(model, optimizer, opt, epoch, save_file)\n\n    # save the last model\n    save_file = os.path.join(\n        opt.save_folder, 'last.pth')\n    save_model(model, optimizer, opt, opt.epochs, save_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport math\nimport argparse\nimport time\nimport torch\n\n# Define an Opt class to hold the configuration\nclass Opt:\n    def __init__(self, **kwargs):\n        # Default values as per the parse_option function\n        self.print_freq = kwargs.get('print_freq', 10)\n        self.save_freq = kwargs.get('save_freq', 50)\n        self.batch_size = kwargs.get('batch_size', 256)\n        self.num_workers = kwargs.get('num_workers', 16)\n        self.epochs = kwargs.get('epochs', 1000)\n\n        # optimization\n        self.learning_rate = kwargs.get('learning_rate', 0.05)\n        self.lr_decay_epochs = kwargs.get('lr_decay_epochs', '700,800,900')\n        self.lr_decay_rate = kwargs.get('lr_decay_rate', 0.1)\n        self.weight_decay = kwargs.get('weight_decay', 1e-4)\n        self.momentum = kwargs.get('momentum', 0.9)\n\n        # model dataset\n        self.model = kwargs.get('model', 'resnet50')\n        self.dataset = kwargs.get('dataset', 'cifar10')\n        self.mean = kwargs.get('mean', None)\n        self.std = kwargs.get('std', None)\n        self.data_folder = kwargs.get('data_folder', None)\n        self.size = kwargs.get('size', 32)\n\n        # method\n        self.method = kwargs.get('method', 'SupCon')\n\n        # temperature\n        self.temp = kwargs.get('temp', 0.07)\n\n        # other settings\n        self.cosine = kwargs.get('cosine', False)\n        self.syncBN = kwargs.get('syncBN', False)\n        self.warm = kwargs.get('warm', False)\n        self.trial = kwargs.get('trial', '0')\n        self.optimizer_name = kwargs.get('optimizer_name', 'adamw')\n\n        # Check if dataset is 'path' and passed required arguments\n        if self.dataset == 'path':\n            assert self.data_folder is not None and self.mean is not None and self.std is not None\n\n        # Set the path according to the environment\n        if self.data_folder is None:\n            self.data_folder = './datasets/'\n\n        self.model_path = './save/SupCon/{}_models'.format(self.dataset)\n        self.tb_path = './save/SupCon/{}_tensorboard'.format(self.dataset)\n\n        # Handle lr_decay_epochs\n        iterations = self.lr_decay_epochs.split(',')\n        self.lr_decay_epochs = [int(it) for it in iterations]\n\n        # Prepare model name\n        self.model_name = '{}_{}_{}_lr_{}_decay_{}_bsz_{}_temp_{}_trial_{}'.format(\n            self.method, self.dataset, self.model, self.learning_rate,\n            self.weight_decay, self.batch_size, self.temp, self.trial)\n\n        if self.cosine:\n            self.model_name = '{}_cosine'.format(self.model_name)\n\n        # Warm-up for large-batch training\n        if self.batch_size > 256:\n            self.warm = True\n        if self.warm:\n            self.model_name = '{}_warm'.format(self.model_name)\n            self.warmup_from = 0.01\n            self.warm_epochs = 10\n            if self.cosine:\n                eta_min = self.learning_rate * (self.lr_decay_rate ** 3)\n                self.warmup_to = eta_min + (self.learning_rate - eta_min) * (\n                        1 + math.cos(math.pi * self.warm_epochs / self.epochs)) / 2\n            else:\n                self.warmup_to = self.learning_rate\n\n        self.tb_folder = os.path.join(self.tb_path, self.model_name)\n        if not os.path.isdir(self.tb_folder):\n            os.makedirs(self.tb_folder)\n\n        self.save_folder = os.path.join(self.model_path, self.model_name)\n        if not os.path.isdir(self.save_folder):\n            os.makedirs(self.save_folder)\n\n# torch.cuda.empty_cache()\n# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\nopt = Opt(\n    batch_size=8,\n    learning_rate=1e-5, \n    print_freq=100,\n    save_freq=100,\n    temp=0.05,\n    cosine=True,\n    num_workers=4,\n    dataset=\"path\",\n    data_folder=\"/kaggle/input/identification-of-pseudopapilledema\",\n    mean= \"(0.4914, 0.4822, 0.4465)\",  \n    std=\"(0.2675, 0.2565, 0.2761)\",   \n    epochs=100,\n    # lr_decay_epochs=\"5,7,9\",  \n    # lr_decay_rate=0.1,\n    # weight_decay=5e-2, \n    size=224,\n    optimizer_name='adamw',\n    model='vit_l_dino',\n    warm=True,  \n)\n\nmain(opt)","metadata":{"id":"yu5OVrd4Vv6m","outputId":"9543fa74-e18f-415e-ec5e-da704eccbfdd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# opt = Opt(\n#     batch_size=32,\n#     learning_rate=0.5, \n#     save_freq =10,\n#     temp=0.1,\n#     cosine=True,\n#     num_workers=4,\n#     dataset=\"path\",\n#     data_folder=\"/kaggle/input/identification-of-pseudopapilledema\",\n#     mean=\"(0.4914, 0.4822, 0.4465)\",\n#     std=\"(0.2675, 0.2565, 0.2761)\",\n#     method=\"SupCon\",\n#     size = 224,\n#     model='vit_s_dino',\n# )\n\n\n\n# opt = Opt(\n#     batch_size=32,\n#     learning_rate=0.5, \n#     save_freq =10,\n#     temp=0.1,\n#     cosine=True,\n#     num_workers=4,\n#     dataset=\"path\",\n#     data_folder=\"/kaggle/input/identification-of-pseudopapilledema\",\n#     mean=\"(0.4914, 0.4822, 0.4465)\",\n#     std=\"(0.2675, 0.2565, 0.2761)\",\n#     method=\"SupCon\",\n#     size = 32,\n#     model='resnet50',\n# )\n\n# main(opt)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}